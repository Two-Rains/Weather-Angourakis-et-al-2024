# Calibration walkthrough

Load source file containing the R implementation of the Weather model:

```{r}
source("source/weatherModel.R")
```

To generate new data based on a given dataset, we must first be able to estimate the Weather model parameters from said datasets. That is, to find the values of each parameter that can approximate the data of a given year daily series. Once this can be done for each year in the dataset, we can then estimate the hyperparameters as descriptive statistics (i.e., mean and standard deviation, minimum, maximum).

The estimation of the parameters of the solar radiation and temperature submodels (i.e. sinusoid) can be made directly by measuring the year minimum and maximum. We will clarify this process later on. However, the case of precipitation is far from trivial, given the complexity of the algorithm behind it. The workflow to estimate the parameters of the precipitation submodel deserves a demonstration.

## Attempt 1: parameter estimation using `optim()`

Set up six variations of parameter settings of the annual double logistic curve (i.e. plateauValue, inflection1, rate1, inflection2, rate2), the escalonation producing the annual cumulative precipitation curve (i.e. nSamples, maxSampleSize) and annualPrecipitation, assuming length of year of 365 days. Random generator seed used in escalonation is fixed:

```{r}
seed = 0

yearLengthInDays_sim = 365

parValuesDoubleLogistic <- rbind(
  # plateauValue, inflection1, rate1, inflection2, rate2
  c(0.01,         125,         0.3,   245,         0.22),
  c(0.15,         63,          0.55,  195,         0.6),
  c(0.5,          64,          0.05,  261,         0.12),
  c(0.45,         215,         0.01,  276,         0.39),
  c(0.6,          20,          0.38,  254,         0.04),
  c(0.85,         97,          0.24,  219,         0.17)
)

parValuesEscalonation <- rbind(
  # nSamples, maxSampleSize
  c(152, 22),
  c(220, 10),
  c(240, 6),
  c(168, 13),
  c(191, 9),
  c(205, 17)
)

annualSumValues <- c(410, 1050, 636, 320, 1280, 745)

```

Select the first set of parameter values from the `parValuesDoubleLogistic` dataset and generate the corresponding curve with the `getAnnualDoubleLogisticCurve()` function. These points will represent the original state of the model that we aim to reverse engineer from the outcome curve. Plot it.

```{r}
originalParams <- parValuesDoubleLogistic[1, 1:5]

curve <- getAnnualDoubleLogisticCurve(
    plateauValue = originalParams[1], 
    inflection1 = originalParams[2], 
    rate1 = originalParams[3], 
    inflection2 = originalParams[4],
    rate2 = originalParams[5],
    yearLengthInDays = yearLengthInDays_sim)

plot(curve, cex = 2)
```

Define the `initialGuess` vector with your initial parameter guess values. Generate the curve using the `getAnnualDoubleLogisticCurve()` function with the initial guess. Plot it. Notice that our initial guess generates a somewhat "average" cumulative curve.

```{r}
initialGuess <- c(0.5, 100, 0.1, 200, 0.1)  # Initial parameter guess

firstGuessCurve <- getAnnualDoubleLogisticCurve(
    plateauValue = initialGuess[1], 
    inflection1 = initialGuess[2], 
    rate1 = initialGuess[3], 
    inflection2 = initialGuess[4],
    rate2 = initialGuess[5],
    yearLengthInDays = yearLengthInDays_sim)

plot(firstGuessCurve, cex = 2)
```

Define the `objectiveFunc()` function that calculates **the sum of squared differences between the observed data and the predicted values**, generated by the `getAnnualDoubleLogisticCurve()` function with a given parameter setting. Then, use the `optim()` function to estimate the best parameter values by minimizing the objective function.

NOTE: `optim()` using method "L-BFGS-B", see `?optim` or: \> Byrd, R. H., Lu, P., Nocedal, J. and Zhu, C. (1995). A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific Computing, 16, 1190--1208. doi:10.1137/0916069.

```{r}
observedData <- curve

# Objective function to minimize (difference between observed and predicted values or "residual")
objectiveFunc <- function(params) {
  predictedData <- getAnnualDoubleLogisticCurve(params[1], params[2], params[3], params[4], params[5], yearLengthInDays_sim)
  sum((observedData - predictedData)^2)
}

# Use the least squares method to estimate the parameter values
fit <- optim(initialGuess, objectiveFunc,
             method = "L-BFGS-B", 
             lower = c(0, 1, 0.01, 1, 0.01), 
             upper = c(1, 365, 0.9, 365, 0.9))

bestEstimationCurve <- getAnnualDoubleLogisticCurve(fit$par[1], fit$par[2], fit$par[3], fit$par[4], fit$par[5], yearLengthInDays_sim)
```

Plot the original curve (curve) and overlay it with the curve generated using the best estimated parameter values (bestEstimationCurve). The best estimated curve is shown in red.

```{r}
plot(curve, cex = 2)
lines(bestEstimationCurve, col = 'red', lwd = 3)
```

```{r}
parameterComparisonData <- cbind(
  parameter = c('plateauValue', 'inflection1', 'rate1', 'inflection2', 'rate2'),
  original = originalParams,
  estimated = fit$par,
  delta = round(abs(originalParams - fit$par), digits = 6)
)

knitr::kable(parameterComparisonData, 
             format = "html",
             col.names = c("parameter", "original", "estimated", "delta"),
             align = c("l", "c", "c", "c"))
```

We can see that reverse engineering the parameter values of the double logistic curve is relatively straightforward. However, precipitation in the Weather model presents an additional challenge: the continuous cumulative curve is broken down into "steps" through `escalonateCurve()`, which introduces **stochasticity**. We will also add `rescaleCurve()` to the end of the process, in order to approach the curve that would be created by `getPrecipitationOfYear()`.

Let us extend the workflow used above with `getAnnualDoubleLogisticCurve()` to also cover the two additional parameters of `escalonateCurve()` (for now, fix `seed = 0`):

```{r}
originalParams <- c(parValuesDoubleLogistic[1, 1:5], parValuesEscalonation[1, 1:2])

curve <- getAnnualDoubleLogisticCurve(
    plateauValue = originalParams[1], 
    inflection1 = originalParams[2], 
    rate1 = originalParams[3], 
    inflection2 = originalParams[4],
    rate2 = originalParams[5],
    yearLengthInDays = yearLengthInDays_sim)

curve <- escalonateCurve(
  curve,
  nSamples = originalParams[6],
  maxSampleSize = originalParams[7],
  seed = 0)

curve <- rescaleCurve(curve)

plot(curve, type = 'l', lwd = 3)
```

```{r}
initialGuess <- c(0.5, 100, 0.1, 200, 0.1, 180, 15)  # Initial parameter guess

firstGuessCurve <- getAnnualDoubleLogisticCurve(
    plateauValue = initialGuess[1], 
    inflection1 = initialGuess[2], 
    rate1 = initialGuess[3], 
    inflection2 = initialGuess[4],
    rate2 = initialGuess[5],
    yearLengthInDays = yearLengthInDays_sim)

firstGuessCurve <- escalonateCurve(
  firstGuessCurve,
  nSamples = initialGuess[6],
  maxSampleSize = initialGuess[7],
  seed = 0)



firstGuessCurve <- rescaleCurve(firstGuessCurve)

plot(firstGuessCurve, type = 'l', lwd = 3)
```

```{r}
observedData <- curve

# Objective function to minimize (difference between observed and predicted values)
objectiveFunc <- function(params) {
  predictedData <- getAnnualDoubleLogisticCurve(params[1], params[2], params[3], params[4], params[5], yearLengthInDays_sim)
  predictedData <- escalonateCurve(predictedData, nSamples = params[6], maxSampleSize = params[7], seed = 0)
  predictedData <- rescaleCurve(predictedData)
  sum((observedData - predictedData)^2)
}

# Use the least squares method to estimate the parameter values

fit <- optim(initialGuess, objectiveFunc,
             method = "L-BFGS-B", 
             lower = c(0, 1, 0.01, 1, 0.01, 1, 3), 
             upper = c(1, 365, 0.9, 365, 0.9, 365, 30))

bestEstimationCurve <- getAnnualDoubleLogisticCurve(fit$par[1], fit$par[2], fit$par[3], fit$par[4], fit$par[5], yearLengthInDays_sim)
bestEstimationCurve <- escalonateCurve(bestEstimationCurve, nSamples = fit$par[6], maxSampleSize = fit$par[7], seed = 0)
bestEstimationCurve <- rescaleCurve(bestEstimationCurve)
```

```{r}
plot(curve, type = 'l', lwd = 3)
lines(bestEstimationCurve, col = 'red', lwd = 3)
```

```{r}
parameterComparisonData <- cbind(
  parameter = c('plateauValue', 'inflection1', 'rate1', 'inflection2', 'rate2', 'nSamples', 'maxSampleSize'),
  original = originalParams,
  estimated = fit$par,
  delta = round(abs(originalParams - fit$par), digits = 6)
)

knitr::kable(parameterComparisonData, 
             format = "html",
             col.names = c("parameter", "original", "estimated", "delta"),
             align = c("l", "c", "c", "c"))
```

Close, but a much worser fit than obtained with `getAnnualDoubleLogisticCurve()` only. It is possible that fixing `seed` might act against the goodness of fit, by limiting the space of model outcomes explored. Let us retry, this time varying `seed` every time optim runs `objectiveFunc()`.

```{r}
originalParams <- c(parValuesDoubleLogistic[1, 1:5], parValuesEscalonation[1, 1:2])
initialGuess <- c(0.5, 100, 0.1, 200, 0.1, 180, 15)

observedData <- curve

# Objective function to minimize (difference between observed and predicted values)
objectiveFunc <- function(params) {
  predictedData <- getAnnualDoubleLogisticCurve(params[1], params[2], params[3], params[4], params[5], yearLengthInDays_sim)
  aSeed = sample(-999:999, 1)
  predictedData <- escalonateCurve(predictedData, nSamples = params[6], maxSampleSize = params[7], seed = aSeed)
  predictedData <- rescaleCurve(predictedData)
  sum((observedData - predictedData)^2)
}

# Use the least squares method to estimate the parameter values
set.seed(0)
fit <- optim(initialGuess, objectiveFunc,
             method = "L-BFGS-B", 
             lower = c(0, 1, 0.01, 1, 0.01, 1, 3, -999), 
             upper = c(1, 365, 0.9, 365, 0.9, 365, 30, 999))

bestEstimationCurve <- getAnnualDoubleLogisticCurve(fit$par[1], fit$par[2], fit$par[3], fit$par[4], fit$par[5], yearLengthInDays_sim)
bestEstimationCurve <- escalonateCurve(bestEstimationCurve, nSamples = fit$par[6], maxSampleSize = fit$par[7], seed = -399)
bestEstimationCurve <- rescaleCurve(bestEstimationCurve)
```

```{r}
plot(curve, type = 'l')
lines(bestEstimationCurve, col = 'red')
```

```{r}
parameterComparisonData <- cbind(
  parameter = c('plateauValue', 'inflection1', 'rate1', 'inflection2', 'rate2', 'nSamples', 'maxSampleSize'),
  original = originalParams,
  estimated = fit$par,
  delta = round(abs(originalParams - fit$par), digits = 6)
)

knitr::kable(parameterComparisonData, 
             format = "html",
             col.names = c("parameter", "original", "estimated", "delta"),
             align = c("l", "c", "c", "c"))
```
